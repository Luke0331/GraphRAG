import nltk
from typing import List, Dict, Any, Tuple

# --- 1. Evaluation Metrics ---

def calculate_bleu(reference: List[str], candidate: str) -> float:
    """
    Calculates the BLEU-4 score for a candidate sentence against reference sentences.
    
    Args:
        reference (List[str]): A list of reference sentences.
        candidate (str): The candidate sentence generated by the model.
        
    Returns:
        float: The BLEU-4 score.
    """
    reference_tokens = [ref.split() for ref in reference]
    candidate_tokens = candidate.split()
    
    # Using smoothing_function to handle cases with no 4-gram overlaps
    chencherry = nltk.translate.bleu_score.SmoothingFunction()
    score = nltk.translate.bleu_score.sentence_bleu(
        reference_tokens, 
        candidate_tokens, 
        smoothing_function=chencherry.method1
    )
    return score

def calculate_recall(retrieved_items: List[Any], ground_truth_items: List[Any]) -> float:
    """
    Calculates the recall for a set of retrieved items against a ground truth set.
    
    Args:
        retrieved_items (List[Any]): The items retrieved by the system.
        ground_truth_items (List[Any]): The expected items.
        
    Returns:
        float: The recall score.
    """
    retrieved_set = set(retrieved_items)
    ground_truth_set = set(ground_truth_items)
    
    true_positives = len(retrieved_set.intersection(ground_truth_set))
    
    if not ground_truth_set:
        return 0.0
        
    recall = true_positives / len(ground_truth_set)
    return recall

def calculate_path_accuracy(retrieved_path: List[str], ground_truth_path: List[str]) -> float:
    """
    Calculates the accuracy of a retrieved path against a ground truth path.
    A simple exact match is used here.
    
    Args:
        retrieved_path (List[str]): The path retrieved by the system (e.g., ['entity1', 'RELATION', 'entity2']).
        ground_truth_path (List[str]): The expected path.
        
    Returns:
        float: 1.0 if paths match, 0.0 otherwise.
    """
    return 1.0 if retrieved_path == ground_truth_path else 0.0


# --- 2. Evaluation Runner ---

class Evaluator:
    def __init__(self, retriever, ground_truth_data: List[Dict[str, Any]]):
        """
        Initializes the evaluator.
        
        Args:
            retriever: The retriever object (e.g., EnhancedHybridRetriever).
            ground_truth_data (List[Dict[str, Any]]): A list of dictionaries,
                                                     each containing a test case.
        """
        self.retriever = retriever
        self.ground_truth_data = ground_truth_data

    def run_evaluation(self):
        """
        Runs the full evaluation suite.
        """
        results = []
        for i, test_case in enumerate(self.ground_truth_data):
            print(f"--- Running Evaluation Case {i+1}/{len(self.ground_truth_data)} ---")
            print(f"Query: {test_case['query']}")
            
            case_results = {"query": test_case["query"]}
            
            # Evaluate Hybrid Search
            hybrid_res = self.retriever.hybrid_search(test_case["query"])
            case_results["hybrid_search"] = self._evaluate_single_run(
                hybrid_res.get("final_answer", ""),
                hybrid_res.get("graph_results", {}).get("relationships", []),
                test_case
            )

            # Evaluate Graph-Guided Retrieval
            guided_res = self.retriever.graph_guided_retrieval(test_case["query"])
            case_results["graph_guided"] = self._evaluate_single_run(
                guided_res.get("final_answer_and_sources", {}).get("answer", ""),
                guided_res.get("graph_search_results", {}).get("results", []),
                test_case
            )
            
            results.append(case_results)
        
        return results

    def _evaluate_single_run(self, answer: str, graph_rels: List[Dict], test_case: Dict) -> Dict:
        """Helper to evaluate one run of a retriever."""
        # BLEU Score
        bleu = calculate_bleu(test_case["reference_answers"], answer)
        
        # Recall for Graph Search
        retrieved_entities = {rel.get("target") for rel in graph_rels if rel.get("target")}
        recall = calculate_recall(list(retrieved_entities), test_case.get("expected_entities", []))
        
        # Path Accuracy (if applicable)
        # This is a simplified check. A real scenario would need to extract the path from results.
        path_acc = 0.0
        if "expected_path" in test_case:
            # You would need a function to extract the path from the 'answer' or 'graph_rels'
            retrieved_path = [] # Placeholder
            path_acc = calculate_path_accuracy(retrieved_path, test_case["expected_path"])

        return {
            "answer": answer,
            "bleu_4": bleu,
            "graph_recall": recall,
            "path_accuracy": path_acc
        }

# --- 3. Sample Data and Main Execution ---

def get_sample_ground_truth():
    """
    Provides a small sample of ground truth data for demonstration.
    """
    return [
        {
            "query": "What material improves the efficiency of crystalline silicon solar cells?",
            "reference_answers": [
                "Materials like silicon nitride and titanium dioxide are used as anti-reflection coatings to improve the efficiency of crystalline silicon solar cells.",
                "Doping crystalline silicon with elements such as phosphorus or boron enhances its electrical properties and thus its efficiency."
            ],
            "expected_entities": ["silicon nitride", "titanium dioxide", "phosphorus", "boron"],
            "expected_path": ["crystalline silicon solar cells", "IMPROVED_BY", "silicon nitride"] # Example for path accuracy
        },
        {
            "query": "How is the performance of a solar cell measured?",
            "reference_answers": [
                "The performance of a solar cell is typically measured by its conversion efficiency, fill factor, open-circuit voltage, and short-circuit current.",
                "Conversion efficiency is a key metric used to evaluate solar cell performance, and it is measured under standard test conditions."
            ],
            "expected_entities": ["conversion efficiency", "fill factor", "open-circuit voltage", "short-circuit current"]
        }
    ]

if __name__ == '__main__':
    # This is a dummy main for demonstration.
    # To run this, you would need to initialize the EnhancedHybridRetriever.
    
    print("--- Testing Evaluation Functions ---")
    
    # Test BLEU
    ref = ["The cat is on the mat"]
    cand = "The cat sat on the mat"
    bleu_score = calculate_bleu(ref, cand)
    print(f"BLEU Score: {bleu_score:.4f} (Example)")
    
    # Test Recall
    retrieved = ["a", "b", "c"]
    truth = ["a", "b", "d", "e"]
    recall_score = calculate_recall(retrieved, truth)
    print(f"Recall: {recall_score:.4f} (Example)")

    # Test Path Accuracy
    path1 = ["a", "R1", "b"]
    path2 = ["a", "R1", "b"]
    path_acc = calculate_path_accuracy(path1, path2)
    print(f"Path Accuracy: {path_acc} (Example)")
    
    print("\nTo run the full evaluator, you need to integrate this with your main application or a separate evaluation script.")
    
    # Example of how you might run the full evaluator
    # from enhanced_hybrid_retriever import EnhancedHybridRetriever
    # retriever = EnhancedHybridRetriever(...) 
    # ground_truth = get_sample_ground_truth()
    # evaluator = Evaluator(retriever, ground_truth)
    # evaluation_results = evaluator.run_evaluation()
    # import json
    # print(json.dumps(evaluation_results, indent=2))
