import streamlit as st
import json
import pandas as pd # Import pandas for dataframes
import streamlit.components.v1 as components
from pyvis.network import Network
from enhanced_hybrid_retriever import EnhancedHybridRetriever
from evaluation import Evaluator, get_sample_ground_truth # Import evaluation tools

# --- 1. åˆå§‹åŒ–ä¸ç¼“å­˜ ---

# ä½¿ç”¨Streamlitçš„ç¼“å­˜åŠŸèƒ½ï¼Œé¿å…æ¯æ¬¡äº¤äº’éƒ½é‡æ–°åŠ è½½æ¨¡å‹
@st.cache_resource
def get_retriever():
    """
    åŠ è½½å¹¶ç¼“å­˜EnhancedHybridRetrieverå®ä¾‹ã€‚
    """
    # ç¡®ä¿æ‚¨çš„APIå¯†é’¥å’Œå¯†ç æ˜¯å®‰å…¨çš„ï¼Œä¾‹å¦‚ä½¿ç”¨Streamlitçš„secretsç®¡ç†
    # è¿™é‡Œä¸ºäº†æ¼”ç¤ºæ–¹ä¾¿ï¼Œç›´æ¥å†™åœ¨ä»£ç é‡Œ
    retriever = EnhancedHybridRetriever(
        neo4j_password="domain-kg123",
        llm_api_key="41b29e65745d4110a018c5d616b0012f.A6CEwmornnYXSVLC" # è¯·æ›¿æ¢ä¸ºæ‚¨çš„æ™ºè°±AI API Key
    )
    return retriever

# --- 2. UIç•Œé¢æ„å»º ---

def main():
    """
    Streamlitåº”ç”¨ä¸»å‡½æ•°
    """
    st.set_page_config(page_title="GraphRAG çŸ¥è¯†æ£€ç´¢ç³»ç»Ÿ", layout="wide")
    
    st.sidebar.title("å¯¼èˆª")
    page = st.sidebar.radio("é€‰æ‹©ä¸€ä¸ªé¡µé¢:", [
        "ğŸ” äº¤äº’å¼æ£€ç´¢", 
        "ğŸ“Š ç³»ç»Ÿè¯„ä¼°"
    ])

    st.title("ğŸ”¬ é¢†åŸŸçŸ¥è¯†å›¾è°±å¢å¼ºçš„RAGç³»ç»Ÿ")
    st.markdown("ä¸€ä¸ªç»“åˆäº†å‘é‡æ£€ç´¢ä¸çŸ¥è¯†å›¾è°±çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼Œç”¨äºå¤ªé˜³èƒ½é¢†åŸŸçš„æ–‡çŒ®ç ”ç©¶ã€‚")

    # åˆå§‹åŒ–retriever
    try:
        retriever = get_retriever()
        # å®‰å…¨åœ°æ£€æŸ¥API keyï¼Œé˜²æ­¢NoneTypeé”™è¯¯
        api_key = getattr(getattr(retriever.langchain_rag.llm, '_llm', None), 'api_key', None)
        if api_key and "YOUR_ZHIPU_API_KEY" in api_key:
            st.error("è¯·åœ¨ `app.py` ä»£ç ä¸­æ›¿æ¢ä¸ºæ‚¨çš„æ™ºè°±AI API Keyã€‚")
            return
    except Exception as e:
        st.error(f"åˆå§‹åŒ–æ£€ç´¢å™¨å¤±è´¥: {e}")
        st.error("è¯·ç¡®ä¿Neo4jæ•°æ®åº“æ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸”API Keyæœ‰æ•ˆã€‚")
        return

    if page == "ğŸ” äº¤äº’å¼æ£€ç´¢":
        run_interactive_retrieval(retriever)
    elif page == "ğŸ“Š ç³»ç»Ÿè¯„ä¼°":
        run_system_evaluation(retriever)


def run_interactive_retrieval(retriever):
    """
    è¿è¡Œäº¤äº’å¼æ£€ç´¢é¡µé¢
    """
    st.sidebar.header("âš™ï¸ æ£€ç´¢è®¾ç½®")
    retrieval_mode = st.sidebar.radio(
        "é€‰æ‹©æ£€ç´¢æ¨¡å¼:",
        ('Graph-Guided Retrieval (é¡ºåºå¼•å¯¼)', 'Hybrid Search (å¹¶è¡Œèåˆ)'),
        help="**é¡ºåºå¼•å¯¼**: é—®é¢˜â†’å›¾æ£€ç´¢â†’å‘é‡ç²¾æ’â†’LLMç”Ÿæˆ (æ›´ç²¾ç¡®)ã€‚\n\n**å¹¶è¡Œèåˆ**: åŒæ—¶è¿›è¡Œå‘é‡å’Œå›¾è°±æ£€ç´¢ï¼Œç„¶åèåˆç»“æœ (å¯èƒ½æ›´å¿«)ã€‚"
    )
    
    top_k = st.sidebar.slider("è¿”å›Top-Kæ–‡æ¡£:", min_value=1, max_value=10, value=3)
    graph_limit = st.sidebar.slider("è¿”å›å›¾è°±å…³ç³»æ•°é‡:", min_value=5, max_value=50, value=15, help="è®¾ç½®ä»çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢çš„æœ€å¤§å…³ç³»/å®ä½“æ•°é‡ã€‚")

    st.header("ğŸ’¬ æå‡ºæ‚¨çš„é—®é¢˜")
    query = st.text_input("ä¾‹å¦‚ï¼šä»€ä¹ˆææ–™å¯ä»¥æé«˜æ™¶ä½“ç¡…å¤ªé˜³èƒ½ç”µæ± çš„æ•ˆç‡ï¼Ÿ", "")

    if st.button("ğŸš€ å¼€å§‹æ£€ç´¢"):
        if not query:
            st.warning("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚")
        else:
            with st.spinner('æ­£åœ¨æ£€ç´¢ä¸­ï¼Œè¯·ç¨å€™...'):
                if retrieval_mode == 'Graph-Guided Retrieval (é¡ºåºå¼•å¯¼)':
                    results = retriever.graph_guided_retrieval(query, top_k=top_k, graph_limit=graph_limit)
                    display_graph_guided_results(results)
                else: # Hybrid Search
                    results = retriever.hybrid_search(query, top_k=top_k, graph_limit=graph_limit)
                    display_hybrid_results(results)

def run_system_evaluation(retriever):
    """
    è¿è¡Œç³»ç»Ÿè¯„ä¼°é¡µé¢
    """
    st.header("ğŸ“Š ç³»ç»Ÿæ€§èƒ½è¯„ä¼°")
    st.markdown("åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªé¢„å®šä¹‰çš„â€œæ ‡å‡†ç­”æ¡ˆâ€æ•°æ®é›†æ¥è¯„ä¼°å’Œæ¯”è¾ƒä¸¤ç§æ£€ç´¢æ¨¡å¼çš„æ€§èƒ½ã€‚")

    ground_truth_data = get_sample_ground_truth()
    
    with st.expander("æŸ¥çœ‹è¯„ä¼°æ•°æ®é›†"):
        st.json(ground_truth_data)

    if st.button("ğŸš€ è¿è¡Œè¯„ä¼°å¥—ä»¶"):
        with st.spinner("æ­£åœ¨è¿è¡Œè¯„ä¼°... è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´..."):
            evaluator = Evaluator(retriever, ground_truth_data)
            results = evaluator.run_evaluation()

            st.success("è¯„ä¼°å®Œæˆï¼")
            st.subheader("ğŸ“ˆ è¯„ä¼°ç»“æœæ‘˜è¦")

            # Prepare data for dataframe
            summary_data = []
            for res in results:
                summary_data.append({
                    "Query": res['query'],
                    "Method": "Hybrid Search",
                    "BLEU-4": res['hybrid_search']['bleu_4'],
                    "Graph Recall": res['hybrid_search']['graph_recall']
                })
                summary_data.append({
                    "Query": res['query'],
                    "Method": "Graph-Guided",
                    "BLEU-4": res['graph_guided']['bleu_4'],
                    "Graph Recall": res['graph_guided']['graph_recall']
                })
            
            df = pd.DataFrame(summary_data)
            st.dataframe(df.style.format({"BLEU-4": "{:.4f}", "Graph Recall": "{:.4f}"}))
            
            st.subheader("ğŸ“‹ è¯¦ç»†ç»“æœ")
            st.json(results)

# --- 3. ç»“æœæ˜¾ç¤ºå‡½æ•° ---

def display_graph_guided_results(results):
    """
    æ˜¾ç¤ºGraph-Guided Retrievalçš„ç»“æœ
    """
    st.success("é¡ºåºå¼•å¯¼æ£€ç´¢å®Œæˆï¼")
    
    if "error" in results:
        st.error(f"æ£€ç´¢å¤±è´¥: {results['error']}")
        return

    # å±•ç¤ºæ¨ç†è·¯å¾„
    st.subheader("ğŸ§  æ¨ç†è·¯å¾„")
    
    path_data = {
        "1. åŸå§‹é—®é¢˜": results.get("original_query"),
        "1.5. è‹±æ–‡ç¿»è¯‘": results.get("english_query"),
        "2. è§£æåçš„æ„å›¾": results.get("parsed_query"),
        "3. ç”Ÿæˆçš„CypheræŸ¥è¯¢": results.get("graph_search_results", {}).get("cypher_query"),
        "4. å›¾è°±æ£€ç´¢ç»“æœ": results.get("graph_search_results", {}).get("results"),
        "5. ç²¾ç‚¼åç”¨äºLLMçš„æŸ¥è¯¢": results.get("refined_query_for_llm"),
        "6. æœ€ç»ˆç­”æ¡ˆ": results.get("final_answer_and_sources", {}).get("answer")
    }

    # ä½¿ç”¨expanderæ¥å±•ç¤ºæ¯ä¸€æ­¥çš„ç»†èŠ‚
    with st.expander("ç‚¹å‡»æŸ¥çœ‹è¯¦ç»†æ¨ç†æ­¥éª¤", expanded=True):
        st.markdown(f"**1. åŸå§‹é—®é¢˜:**\n`{path_data['1. åŸå§‹é—®é¢˜']}`")
        st.markdown(f"**1.5. è‹±æ–‡ç¿»è¯‘:**\n`{path_data['1.5. è‹±æ–‡ç¿»è¯‘']}`")
        st.markdown("**2. é—®é¢˜è§£æ:**")
        st.json(path_data["2. è§£æåçš„æ„å›¾"])
        st.markdown(f"**3. åŠ¨æ€ç”Ÿæˆçš„CypheræŸ¥è¯¢:**")
        st.code(path_data["3. ç”Ÿæˆçš„CypheræŸ¥è¯¢"], language="cypher")
        st.markdown("**4. å›¾è°±è¿”å›çš„å…³é”®äº‹å®:**")
        st.json(path_data["4. å›¾è°±æ£€ç´¢ç»“æœ"])
        st.markdown("**5. ç”¨äºæŒ‡å¯¼LLMçš„ç²¾ç‚¼æŸ¥è¯¢:**")
        st.info(path_data["5. ç²¾ç‚¼åç”¨äºLLMçš„æŸ¥è¯¢"])

    st.subheader("âœ… æœ€ç»ˆç­”æ¡ˆ")
    st.markdown(path_data["6. æœ€ç»ˆç­”æ¡ˆ"])

    # æ˜¾ç¤ºå›¾è°±å¯è§†åŒ–
    if path_data["4. å›¾è°±æ£€ç´¢ç»“æœ"]:
        with st.expander("çŸ¥è¯†å›¾è°±å¯è§†åŒ–", expanded=True):
            generate_and_display_graph(path_data["4. å›¾è°±æ£€ç´¢ç»“æœ"])
    
    # æ˜¾ç¤ºæºæ–‡æ¡£
    display_source_documents(results.get("final_answer_and_sources", {}))


def display_hybrid_results(results):
    """
    æ˜¾ç¤ºHybrid Searchçš„ç»“æœ
    """
    st.success("å¹¶è¡Œèåˆæ£€ç´¢å®Œæˆï¼")
    
    # æœ€ç»ˆç­”æ¡ˆ
    st.subheader("âœ… æœ€ç»ˆç­”æ¡ˆ")
    st.markdown(results.get("final_answer", "æœªèƒ½ç”Ÿæˆç­”æ¡ˆã€‚"))

    # ä¸¤è·¯ç»“æœå¯¹æ¯”
    st.subheader("ğŸ” åŒè·¯æ£€ç´¢ç»“æœå¯¹æ¯”")
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("#### ğŸ“š å‘é‡æ£€ç´¢")
        vector_res = results.get("vector_results", {})
        st.metric("æ‰§è¡Œæ—¶é—´ (ç§’)", f"{vector_res.get('execution_time', 0):.2f}")
        with st.expander("æŸ¥çœ‹å‘é‡æ£€ç´¢è¯¦æƒ…", expanded=False):
            st.json(vector_res)

    with col2:
        st.markdown("#### ğŸ•¸ï¸ å›¾è°±æ£€ç´¢")
        graph_res = results.get("graph_results", {})
        st.metric("å‘ç°å®ä½“æ•°", graph_res.get("entities_found", 0))
        with st.expander("æŸ¥çœ‹å›¾è°±æ£€ç´¢è¯¦æƒ…", expanded=False):
            st.code(graph_res.get('cypher_query', ''), language='cypher')
            st.json(graph_res.get("relationships", []))
            
    # æ–°å¢ï¼šæ˜¾ç¤ºèåˆæç¤º
    st.subheader("ğŸ¤ ä¸Šä¸‹æ–‡èåˆæç¤º")
    fusion_prompt = results.get("fusion_prompt", "æœªç”Ÿæˆèåˆæç¤ºã€‚")
    with st.expander("ç‚¹å‡»æŸ¥çœ‹å‘é€ç»™LLMçš„æœ€ç»ˆèåˆæç¤º", expanded=False):
        st.code(fusion_prompt, language='markdown')
        
    # æ˜¾ç¤ºå›¾è°±å¯è§†åŒ–
    graph_rels = results.get("graph_results", {}).get("relationships")
    if graph_rels:
        st.subheader("ğŸ¨ çŸ¥è¯†å›¾è°±å¯è§†åŒ–")
        with st.expander("ç‚¹å‡»å±•å¼€/æŠ˜å å›¾è°±", expanded=True):
            generate_and_display_graph(graph_rels)

    # æ˜¾ç¤ºæºæ–‡æ¡£
    display_source_documents(results.get("vector_results", {}))


def generate_and_display_graph(relationships: list):
    """
    ä½¿ç”¨pyvisç”Ÿæˆå¹¶æ˜¾ç¤ºå›¾è°±ã€‚
    """
    if not relationships:
        st.warning("æ²¡æœ‰å¯ä¾›å¯è§†åŒ–çš„å›¾è°±æ•°æ®ã€‚")
        return

    net = Network(height="450px", width="100%", bgcolor="#222222", font_color="white", notebook=True, cdn_resources='in_line')
    
    # è®¾ç½®ç‰©ç†å¼•æ“ä»¥è·å¾—æ›´å¥½çš„å¸ƒå±€
    net.set_options("""
    var options = {
      "physics": {
        "forceAtlas2Based": {
          "gravitationalConstant": -50,
          "centralGravity": 0.01,
          "springLength": 100,
          "springConstant": 0.08
        },
        "maxVelocity": 50,
        "minVelocity": 0.1,
        "solver": "forceAtlas2Based",
        "stabilization": {
          "enabled": true,
          "iterations": 1000
        }
      }
    }
    """)

    # æ·»åŠ èŠ‚ç‚¹å’Œè¾¹
    nodes = set()
    for rel in relationships:
        # å…¼å®¹å¤šç§ç»“æœæ ¼å¼
        source = rel.get("source") or rel.get("found_entity")
        target = rel.get("target") or rel.get("related_to")  # æ–°å¢å¯¹related_toçš„æ”¯æŒ
        label = rel.get("relationship")

        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰targetï¼Œå°è¯•å…¶ä»–å›é€€ç­–ç•¥
        if not target and source:
            target = rel.get("source")  # åŸæ¥çš„å›é€€é€»è¾‘

        if source and target and label:
            if source not in nodes:
                net.add_node(source, label=source, title=source)
                nodes.add(source)
            if target not in nodes:
                net.add_node(target, label=target, title=target)
                nodes.add(target)
            
            net.add_edge(source, target, label=label, title=label)

    # ç”Ÿæˆå¹¶æ˜¾ç¤ºHTML
    try:
        path = 'tmp'
        import os
        if not os.path.exists(path):
            os.makedirs(path)
        
        file_path = os.path.join(path, 'graph.html')
        
        # Manually generate HTML and write with UTF-8 encoding to fix GBK error
        html_content = net.generate_html()
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(html_content)

        with open(file_path, 'r', encoding='utf-8') as f:
            html_data = f.read()
        
        components.html(html_data, height=470)
        
    except Exception as e:
        st.error(f"ç”Ÿæˆå›¾è°±å¯è§†åŒ–å¤±è´¥: {e}")


def display_source_documents(results_with_sources):
    """
    æ˜¾ç¤ºæºæ–‡æ¡£ä¿¡æ¯,å¹¶åˆ©ç”¨æ–°çš„metadataæ ¼å¼æä¾›æ›´ä¸°å¯Œçš„å¼•ç”¨ã€‚
    """
    source_documents = results_with_sources.get("source_documents")
    
    if source_documents:
        with st.expander("ğŸ“š æŸ¥çœ‹å¼•ç”¨çš„æºæ–‡æ¡£"):
            for doc in source_documents:
                # Correctly access data from the dictionary
                metadata = doc.get('metadata', {})
                content = doc.get('content', 'å†…å®¹ä¸å¯ç”¨')
                doc_index = doc.get('index', 0)

                # --- æ„å»ºä¸°å¯Œçš„å¼•ç”¨ä¿¡æ¯ ---
                citation_parts = []
                
                # ä½œè€…
                author = metadata.get('pdf_author')
                if author and author != 'N/A':
                    citation_parts.append(f"**Author(s):** {author}")
                
                # æ ‡é¢˜
                title = metadata.get('pdf_title')
                if title and title != 'N/A':
                    citation_parts.append(f"**Title:** *{title}*")
                
                # æ–‡ä»¶æ¥æºå’Œé¡µç 
                source_file = metadata.get('source_file', 'æœªçŸ¥æ¥æº')
                page_number = metadata.get('page_number', 'N/A')
                citation_parts.append(f"**Source:** `{source_file}` (Page: {page_number})")
                
                # å—ç±»å‹
                chunk_type = metadata.get('chunk_type', 'text').capitalize()
                citation_parts.append(f"**Type:** {chunk_type}")

                # æœ€ç»ˆå¼•ç”¨å­—ç¬¦ä¸²
                citation = " | ".join(citation_parts)
                st.markdown(citation)

                # --- æ˜¾ç¤ºå†…å®¹ç‰‡æ®µ ---
                st.text_area(
                    "å†…å®¹ç‰‡æ®µ:", 
                    content, 
                    height=150, 
                    key=f"doc_{doc_index}_{source_file}_{page_number}" # Improved key for uniqueness
                )
                st.divider()


if __name__ == "__main__":
    main()
