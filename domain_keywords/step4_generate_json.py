#!/usr/bin/env python3
"""
ç¬¬4æ­¥ï¼šè‡ªåŠ¨ç”Ÿæˆ - ä»è¡¨æ ¼ä¸€é”®ç”Ÿæˆæœ€ç»ˆçš„JSONæ–‡ä»¶
å°†äººå·¥æ•´ç†çš„Excelè¡¨æ ¼è½¬æ¢ä¸ºç»“æ„åŒ–çš„JSONè¯å…¸
"""

import pandas as pd
import json
from pathlib import Path
import logging
from collections import defaultdict

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_curated_data():
    """åŠ è½½äººå·¥æ•´ç†çš„æ•°æ®"""
    excel_file = Path("curated_dictionary.xlsx")
    
    if not excel_file.exists():
        logger.error(f"Excelæ–‡ä»¶ä¸å­˜åœ¨: {excel_file}")
        logger.info("è¯·å…ˆè¿è¡Œ step3_create_spreadsheet.py å¹¶å®Œæˆäººå·¥æ•´ç†")
        return None
    
    try:
        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(excel_file)
        logger.info(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼ŒåŒ…å« {len(df)} è¡Œæ•°æ®")
        return df
    except Exception as e:
        logger.error(f"è¯»å–Excelæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return None

def process_curated_data(df):
    """å¤„ç†äººå·¥æ•´ç†çš„æ•°æ®"""
    # ç­›é€‰å‡ºä¿ç•™çš„è¯æ¡
    df_kept = df[df['Is_Keep'] == 1].copy()
    
    if df_kept.empty:
        logger.error("æ²¡æœ‰æ‰¾åˆ°æ ‡è®°ä¸ºä¿ç•™çš„è¯æ¡")
        return []
    
    logger.info(f"æ‰¾åˆ° {len(df_kept)} ä¸ªä¿ç•™çš„è¯æ¡")
    
    # æŒ‰æ ‡å‡†ååˆ†ç»„
    grouped_data = defaultdict(list)
    
    for _, row in df_kept.iterrows():
        candidate_term = str(row['Candidate_Term']).strip()
        standard_name = str(row['Standard_Name']).strip()
        category = str(row['Category']).strip()
        
        # è·³è¿‡ç©ºçš„æ ‡å‡†å
        if not standard_name or standard_name == 'nan':
            logger.warning(f"è·³è¿‡æ²¡æœ‰æ ‡å‡†åçš„è¯æ¡: {candidate_term}")
            continue
        
        # æ·»åŠ åˆ°åˆ†ç»„ä¸­
        grouped_data[standard_name].append({
            'alias': candidate_term,
            'category': category if category and category != 'nan' else 'Unknown'
        })
    
    # æ„å»ºæœ€ç»ˆè¯å…¸
    final_dictionary = []
    
    for standard_name, aliases_data in grouped_data.items():
        # æ”¶é›†æ‰€æœ‰åˆ«å
        aliases = [item['alias'] for item in aliases_data]
        
        # ç¡®ä¿æ ‡å‡†ååœ¨åˆ«ååˆ—è¡¨ä¸­
        if standard_name not in aliases:
            aliases.append(standard_name)
        
        # è·å–ç±»åˆ«ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªéç©ºçš„ç±»åˆ«ï¼‰
        categories = [item['category'] for item in aliases_data if item['category'] != 'Unknown']
        category = categories[0] if categories else 'Unknown'
        
        entry = {
            "standard_name": standard_name,
            "aliases": sorted(list(set(aliases))),  # å»é‡å¹¶æ’åº
            "category": category
        }
        
        final_dictionary.append(entry)
    
    # æŒ‰æ ‡å‡†åæ’åº
    final_dictionary.sort(key=lambda x: x['standard_name'])
    
    return final_dictionary

def generate_statistics(dictionary):
    """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
    stats = {
        'total_terms': len(dictionary),
        'categories': defaultdict(int),
        'total_aliases': 0
    }
    
    for entry in dictionary:
        stats['categories'][entry['category']] += 1
        stats['total_aliases'] += len(entry['aliases'])
    
    return stats

def save_json_dictionary(dictionary, output_file):
    """ä¿å­˜JSONè¯å…¸"""
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(dictionary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"JSONè¯å…¸å·²ä¿å­˜: {output_file.absolute()}")
        return True
    except Exception as e:
        logger.error(f"ä¿å­˜JSONæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return False

def save_summary_report(dictionary, stats, output_file):
    """ä¿å­˜æ‘˜è¦æŠ¥å‘Š"""
    report = f"""# ç¡…ç”µæ± é¢†åŸŸè¯å…¸ç”ŸæˆæŠ¥å‘Š

## ç»Ÿè®¡ä¿¡æ¯
- æ€»æœ¯è¯­æ•°: {stats['total_terms']}
- æ€»åˆ«åæ•°: {stats['total_aliases']}
- å¹³å‡åˆ«åæ•°: {stats['total_aliases'] / stats['total_terms']:.1f}

## åˆ†ç±»ç»Ÿè®¡
"""
    
    for category, count in sorted(stats['categories'].items()):
        percentage = (count / stats['total_terms']) * 100
        report += f"- {category}: {count} ({percentage:.1f}%)\n"
    
    report += "\n## æœ¯è¯­åˆ—è¡¨\n"
    
    for entry in dictionary:
        report += f"\n### {entry['standard_name']} ({entry['category']})\n"
        report += f"åˆ«å: {', '.join(entry['aliases'])}\n"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info(f"æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜: {output_file.absolute()}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¼€å§‹ç”Ÿæˆæœ€ç»ˆè¯å…¸...")
    
    # åŠ è½½äººå·¥æ•´ç†çš„æ•°æ®
    df = load_curated_data()
    if df is None:
        return
    
    # å¤„ç†æ•°æ®
    dictionary = process_curated_data(df)
    if not dictionary:
        return
    
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    stats = generate_statistics(dictionary)
    
    # ä¿å­˜JSONè¯å…¸
    json_file = Path("domain_dictionary.json")
    if save_json_dictionary(dictionary, json_file):
        logger.info("âœ… è¯å…¸ç”ŸæˆæˆåŠŸï¼")
    
    # ä¿å­˜æ‘˜è¦æŠ¥å‘Š
    report_file = Path("dictionary_report.md")
    save_summary_report(dictionary, stats, report_file)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    logger.info("ğŸ“Š è¯å…¸ç»Ÿè®¡:")
    logger.info(f"   - æ€»æœ¯è¯­æ•°: {stats['total_terms']}")
    logger.info(f"   - æ€»åˆ«åæ•°: {stats['total_aliases']}")
    logger.info(f"   - å¹³å‡åˆ«åæ•°: {stats['total_aliases'] / stats['total_terms']:.1f}")
    
    logger.info("ğŸ“‚ åˆ†ç±»ç»Ÿè®¡:")
    for category, count in sorted(stats['categories'].items()):
        percentage = (count / stats['total_terms']) * 100
        logger.info(f"   - {category}: {count} ({percentage:.1f}%)")
    
    logger.info(f"\nğŸ‰ è¯å…¸ç”Ÿæˆå®Œæˆï¼")
    logger.info(f"ğŸ“„ JSONæ–‡ä»¶: {json_file.absolute()}")
    logger.info(f"ğŸ“‹ æŠ¥å‘Šæ–‡ä»¶: {report_file.absolute()}")

if __name__ == "__main__":
    main() 