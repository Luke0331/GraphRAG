# -*- coding: utf-8 -*-
"""
å¢å¼ºæ··åˆæ£€ç´¢å™¨ï¼šæ•´åˆLangChain RetrievalQAå’Œå›¾è°±æ£€ç´¢
åŸºäºç°æœ‰çš„langchain_retrieval_qa.pyç³»ç»Ÿ
"""

import json
import re
import time
from typing import List, Dict, Any, Optional
from langchain_retrieval_qa import LangChainDomainRAG
from graph_retriever import GraphRetriever
from entity_linker import EntityLinker
from query_parser import QueryParser # I am adding this import

class EnhancedHybridRetriever:
    """
    å¢å¼ºæ··åˆæ£€ç´¢å™¨ï¼šæ•´åˆLangChain RetrievalQAå’Œå›¾è°±æ£€ç´¢
    """
    
    def __init__(self, 
                 domain_dict_path: str = "domain_keywords/domain_dictionary_cleaned.json",
                 vector_store_path: str = "RAG-Wikipedia/dataset/vector_storage_with_metadata",
                 neo4j_uri: str = "bolt://localhost:7687",
                 neo4j_user: str = "neo4j",
                 neo4j_password: str = "domain-kg123",
                 llm_api_key: str = "41b29e65745d4110a018c5d616b0012f.A6CEwmornnYXSVLC",
                 llm_model: str = "glm-4-flash"):
        
        # åˆå§‹åŒ–LangChain RAGç³»ç»Ÿ
        self.langchain_rag = LangChainDomainRAG(
            domain_dict_path=domain_dict_path,
            vector_store_path=vector_store_path,
            llm_api_key=llm_api_key,
            llm_model=llm_model
        )
        
        # åˆå§‹åŒ–å›¾è°±æ£€ç´¢å™¨
        self.graph_retriever = GraphRetriever(neo4j_uri, neo4j_user, neo4j_password)
        
        # åˆå§‹åŒ–å®ä½“é“¾æ¥å™¨
        self.entity_linker = EntityLinker(domain_dict_path)

        # åˆå§‹åŒ–Query Parser
        self.query_parser = QueryParser(entity_linker=self.entity_linker, api_key=llm_api_key)
        
        print("âœ“ å¢å¼ºæ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _is_english(self, text: str) -> bool:
        """
        ç®€å•åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºè‹±æ–‡
        """
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
        return len(chinese_chars) == 0 and len(text.strip()) > 0
    
    def graph_guided_retrieval(self, query: str, top_k: int = 3, graph_limit: int = 15) -> Dict[str, Any]:
        """
        Orchestrates the 'Parse -> Graph Search -> Refine -> Generate' pipeline.
        """
        print(f"\nğŸš€ æ‰§è¡Œå›¾è°±å¼•å¯¼çš„æ£€ç´¢: {query}")
        print("="*50)
        start_time = time.time()

        # 0. ç¿»è¯‘æŸ¥è¯¢ä¸ºè‹±æ–‡ï¼ˆç¡®ä¿å®ä½“åŒ¹é…ï¼‰
        print("0. å‡†å¤‡è‹±æ–‡æŸ¥è¯¢...")
        # å¦‚æœæŸ¥è¯¢å·²ç»æ˜¯è‹±æ–‡ï¼Œå°±ä¸éœ€è¦ç¿»è¯‘
        if self._is_english(query):
            english_query = query
            print(f"   - æŸ¥è¯¢å·²ç»æ˜¯è‹±æ–‡: {english_query}")
        else:
            english_query = self.langchain_rag._translate_to_english(query)
            print(f"   - ç¿»è¯‘åçš„è‹±æ–‡æŸ¥è¯¢: {english_query}")

        # 1. é—®é¢˜è§£æ (Parse)
        print("1. è§£ææŸ¥è¯¢...")
        parsed_query = self.query_parser.parse_query(english_query)
        if parsed_query.get("error"):
            print(f"âœ— æŸ¥è¯¢è§£æå¤±è´¥: {parsed_query.get('error')}")
            return {"error": "Query parsing failed", "details": parsed_query}
        print(f"   - æ„å›¾: {parsed_query.get('intent')}")
        print(f"   - å®ä½“: {parsed_query.get('source_entities')}")

        # 2. å›¾æ£€ç´¢ (Graph Search)
        print("2. æ‰§è¡Œå›¾æ£€ç´¢...")
        graph_results = self.graph_retriever.search_from_parsed_query(parsed_query, limit=graph_limit)
        if graph_results.get("error") or not graph_results.get("results"):
            print("   - ç»“æ„åŒ–å›¾æ£€ç´¢æœªæ‰¾åˆ°ç»“æœï¼Œå°è¯•è‡ªç„¶è¯­è¨€å›¾æ£€ç´¢...")
            # å°è¯•è‡ªç„¶è¯­è¨€å›¾æ£€ç´¢ä½œä¸ºå¤‡ç”¨
            fallback_graph_results = self.graph_retriever.query(english_query)
            if fallback_graph_results.get("results"):
                print(f"   - è‡ªç„¶è¯­è¨€å›¾æ£€ç´¢æ‰¾åˆ° {len(fallback_graph_results['results'])} ä¸ªç»“æœ")
                graph_results = fallback_graph_results
            else:
                print("   - æ‰€æœ‰å›¾æ£€ç´¢æ–¹æ³•éƒ½æœªæ‰¾åˆ°ç»“æœï¼Œå°†å›é€€åˆ°æ ‡å‡†å‘é‡æ£€ç´¢ã€‚")
                # Fallback to standard vector search if graph search fails
                return self.langchain_vector_search(query, top_k=top_k)
        
        print(f"   - Cypher æŸ¥è¯¢: {graph_results.get('cypher_query', '').strip()}")
        print(f"   - æ‰¾åˆ° {graph_results.get('count')} ä¸ªå›¾è°±ç»“æœã€‚")

        # 3. å‘é‡ç²¾æ’ (Vector Refinement via Query Generation)
        # We refine the vector search by giving it a much more specific query
        # synthesized from the graph results.
        print("3. åŸºäºå›¾è°±ç»“æœç”Ÿæˆç²¾ç¡®æŸ¥è¯¢ä»¥è¿›è¡Œç²¾æ’...")
        refined_query = self._synthesize_query_from_graph(query, graph_results)
        print(f"   - ç²¾ç‚¼åçš„æŸ¥è¯¢: \"{refined_query}\"")

        # 4. LLMç”Ÿæˆ (LLM Generation)
        # Use the refined query to get the final answer from the LLM.
        print("4. ä½¿ç”¨ç²¾ç¡®æŸ¥è¯¢è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...")
        final_result = self.langchain_vector_search(
            refined_query, 
            top_k=top_k, 
            use_query_expansion=False, # Expansion is not needed for our specific query
            use_query_rewriting=False
        )

        execution_time = time.time() - start_time
        print(f"â±ï¸ å›¾è°±å¼•å¯¼æ£€ç´¢æ€»æ‰§è¡Œæ—¶é—´: {execution_time:.2f}s")
        print("="*50)

        # Combine all information for a comprehensive output
        return {
            "pipeline": "graph_guided_retrieval",
            "original_query": query,
            "english_query": english_query,
            "parsed_query": parsed_query,
            "graph_search_results": graph_results,
            "refined_query_for_llm": refined_query,
            "final_answer_and_sources": final_result,
            "execution_time": execution_time
        }

    def _synthesize_query_from_graph(self, original_query: str, graph_results: Dict[str, Any]) -> str:
        """
        Creates a new, more specific query for the LLM based on graph findings.
        """
        graph_findings = []
        for res in graph_results.get("results", []):
            # Handles 'find_relation' results
            if "source" in res and "target" in res:
                finding = f"{res['source']} -> {res['relationship']} -> {res['target']}"
            # Handles results from our new constrained search
            elif "found_entity" in res and "related_to" in res:
                entity = res['found_entity']
                entity_type = res.get('entity_type', ['Unknown'])[0] # Safely get type
                relation = res['relationship']
                related_to = res['related_to']
                finding = f"Found a '{entity_type}' named '{entity}', which '{relation}' a metric related to '{related_to}'."
            # Handles 'find_entity' results
            elif "found_entity" in res:
                finding = f"Found entity '{res['found_entity']}' related via '{res['relationship']}'"
            else:
                continue
            graph_findings.append(finding)

        if not graph_findings:
            findings_str = "No specific facts were found in the knowledge graph for this query."
        else:
            findings_str = "\n".join([f"- {f}" for f in graph_findings])


        # This prompt asks the LLM to synthesize an answer using the graph data as a hard context.
        refined_query = f"""
        Original question: "{original_query}"

        From a knowledge graph, I have retrieved the following verified facts:
        {findings_str}

        Based *only* on these facts and the documents related to them, please provide a comprehensive answer to the original question. Summarize the findings and explain how these facts answer the question.
        """
        return refined_query.strip()

    def langchain_vector_search(self, query: str, 
                              use_query_expansion: bool = True, 
                              use_query_rewriting: bool = True,
                              top_k: int = 5,
                              prompt_type: str = "qa",
                              use_structured_query: bool = False) -> Dict[str, Any]:
        """
        ä½¿ç”¨LangChainè¿›è¡Œå‘é‡æ£€ç´¢
        """
        try:
            # è°ƒç”¨LangChain RAGç³»ç»Ÿçš„queryæ–¹æ³•
            result = self.langchain_rag.query(
                query,
                use_query_expansion=use_query_expansion,
                use_query_rewriting=use_query_rewriting,
                top_k=top_k,
                prompt_type=prompt_type,
                use_structured_query=use_structured_query
            )
            
            return {
                "method": "langchain_vector_search",
                "query": query,
                "answer": result.get('answer', ''),
                "original_query": result.get('original_query', ''),
                "expanded_query": result.get('expanded_query', ''),
                "final_query": result.get('final_query', ''),
                "domain_terms": result.get('domain_terms', []),
                "execution_time": result.get('execution_time', 0),
                "framework": result.get('framework', 'LangChain RetrievalQA'),
                "top_k_used": top_k,
                "prompt_type_used": prompt_type,
                "use_structured_query": use_structured_query,
                "source_documents": result.get('source_documents', []),
                "retrieved_docs_count": result.get('retrieved_docs_count', 0)
            }
        except Exception as e:
            print(f"âœ— LangChainå‘é‡æ£€ç´¢å¤±è´¥: {e}")
            return {
                "method": "langchain_vector_search",
                "query": query,
                "answer": "æ£€ç´¢å¤±è´¥",
                "error": str(e),
                "execution_time": 0,
                "framework": "LangChain RetrievalQA",
                "top_k_used": top_k,
                "prompt_type_used": prompt_type,
                "use_structured_query": use_structured_query,
                "source_documents": [],
                "retrieved_docs_count": 0
            }
    
    def graph_knowledge_search(self, query: str, limit: int = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨å›¾è°±è¿›è¡ŒçŸ¥è¯†æ£€ç´¢ï¼ˆä½¿ç”¨ç»“æ„åŒ–æŸ¥è¯¢è§£æï¼‰
        """
        try:
            # ä½¿ç”¨ä¸graph_guided_retrievalç›¸åŒçš„é€»è¾‘
            print(f"ğŸ•¸ï¸ æ‰§è¡Œç»“æ„åŒ–å›¾è°±æ£€ç´¢: {query}")
            
            # ç¿»è¯‘æŸ¥è¯¢ä¸ºè‹±æ–‡ï¼ˆç¡®ä¿å®ä½“åŒ¹é…ï¼‰
            if self._is_english(query):
                english_query = query
                print(f"   - æŸ¥è¯¢å·²ç»æ˜¯è‹±æ–‡: {english_query}")
            else:
                english_query = self.langchain_rag._translate_to_english(query)
                print(f"   - ç¿»è¯‘åçš„è‹±æ–‡æŸ¥è¯¢: {english_query}")

            # è§£ææŸ¥è¯¢
            print("   - è§£ææŸ¥è¯¢...")
            parsed_query = self.query_parser.parse_query(english_query)
            if parsed_query.get("error"):
                print(f"   - æŸ¥è¯¢è§£æå¤±è´¥: {parsed_query.get('error')}")
                # å›é€€åˆ°æ—§æ–¹æ³•
                graph_results = self.graph_retriever.query(english_query, limit=limit)
            else:
                print(f"   - è§£ææˆåŠŸï¼Œæ„å›¾: {parsed_query.get('intent')}")
                # ä½¿ç”¨ç»“æ„åŒ–æŸ¥è¯¢
                graph_results = self.graph_retriever.search_from_parsed_query(parsed_query, limit=limit if limit is not None else 15)
            
            # æå–æŸ¥è¯¢ä¸­çš„å®ä½“ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
            entities = self.entity_linker.extract_entities_from_text(query)
            
            return {
                "method": "structured_graph_retrieval",
                "query": query,
                "english_query": english_query,
                "parsed_query": parsed_query,
                "extracted_entities": entities,
                "graph_results": graph_results,
                "cypher_query": graph_results.get('cypher_query', ''),
                "entities_found": len(graph_results.get('results', [])),
                "relationships": graph_results.get('results', []),
                "limit_used": limit if limit is not None else 15
            }
        except Exception as e:
            print(f"âœ— ç»“æ„åŒ–å›¾è°±æ£€ç´¢å¤±è´¥: {e}")
            # å›é€€åˆ°æ—§æ–¹æ³•
            try:
                entities = self.entity_linker.extract_entities_from_text(query)
                graph_results = self.graph_retriever.query(query, limit=limit)
                return {
                    "method": "fallback_graph_retrieval",
                    "query": query,
                    "extracted_entities": entities,
                    "graph_results": graph_results,
                    "cypher_query": graph_results.get('cypher_query', ''),
                    "entities_found": graph_results.get('count', 0),
                    "relationships": graph_results.get('results', []),
                    "limit_used": graph_results.get('limit_used', limit if limit is not None else 50)
                }
            except Exception as fallback_error:
                print(f"âœ— å›é€€å›¾è°±æ£€ç´¢ä¹Ÿå¤±è´¥: {fallback_error}")
                return {
                    "method": "graph_retrieval",
                    "query": query,
                    "error": str(e),
                    "entities_found": 0,
                    "relationships": [],
                    "limit_used": limit if limit is not None else 50
                }
    
    def hybrid_search(self, query: str, 
                     vector_weight: float = 0.6,
                     graph_weight: float = 0.4,
                     use_query_expansion: bool = True,
                     use_query_rewriting: bool = True,
                     graph_limit: int = None,
                     top_k: int = 5,
                     prompt_type: str = "qa",
                     use_structured_query: bool = False) -> Dict[str, Any]:
        """
        æ··åˆæ£€ç´¢ï¼šç»“åˆLangChainå‘é‡æ£€ç´¢å’Œå›¾è°±æ£€ç´¢
        """
        start_time = time.time()
        
        print(f"\nğŸ” æ··åˆæ£€ç´¢æŸ¥è¯¢: {query}")
        print("="*50)
        
        # 1. LangChainå‘é‡æ£€ç´¢
        print("ğŸ“š æ‰§è¡ŒLangChainå‘é‡æ£€ç´¢...")
        vector_results = self.langchain_vector_search(
            query, 
            use_query_expansion=use_query_expansion,
            use_query_rewriting=use_query_rewriting,
            top_k=top_k,
            prompt_type=prompt_type,
            use_structured_query=use_structured_query
        )
        
        # 2. å›¾è°±çŸ¥è¯†æ£€ç´¢
        print("ğŸ•¸ï¸ æ‰§è¡Œå›¾è°±çŸ¥è¯†æ£€ç´¢...")
        graph_results = self.graph_knowledge_search(query, limit=graph_limit)
        
        # 3. èåˆç»“æœ
        execution_time = time.time() - start_time
        
        # æ”¹è¿›çš„è¯„åˆ†è®¡ç®—
        vector_score = self._calculate_vector_score(vector_results, vector_weight)
        graph_score = self._calculate_graph_score(graph_results, graph_weight)
        combined_score = vector_score + graph_score
        
        # è´¨é‡è¯„ä¼°
        quality_metrics = self._assess_quality(vector_results, graph_results)
        
        # 4. ä¸Šä¸‹æ–‡èåˆç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        print("ğŸ¤ æ‰§è¡Œä¸Šä¸‹æ–‡èåˆ...")
        final_answer, fusion_prompt = self._context_fusion_generation(
            query, 
            vector_results, 
            graph_results
        )
        
        hybrid_results = {
            "query": query,
            "vector_results": vector_results,
            "graph_results": graph_results,
            "combined_info": {
                "vector_score": vector_score,
                "graph_score": graph_score,
                "combined_score": combined_score,
                "quality_metrics": quality_metrics,
                "execution_time": execution_time,
                "vector_weight": vector_weight,
                "graph_weight": graph_weight,
                "graph_limit_used": graph_results.get('limit_used', graph_limit if graph_limit is not None else 50),
                "top_k_used": top_k,
                "prompt_type_used": prompt_type,
                "use_structured_query": use_structured_query
            },
            "final_answer": final_answer,
            "fusion_prompt": fusion_prompt
        }
        
        print(f"â±ï¸ æ··åˆæ£€ç´¢æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        print(f"ğŸ“Š ç»¼åˆè¯„åˆ†: {combined_score:.2f}")
        print(f"ğŸ¯ è´¨é‡è¯„ä¼°: {quality_metrics}")
        print(f"ğŸ•¸ï¸ å›¾è°±é™åˆ¶: {graph_results.get('limit_used', 'N/A')}")
        print(f"ğŸ“š å‘é‡æ£€ç´¢top_k: {top_k}")
        print(f"ğŸ­ æç¤ºæ¨¡æ¿ç±»å‹: {prompt_type}")
        print(f"ğŸ§© ç»“æ„åŒ–æŸ¥è¯¢: {use_structured_query}")
        print("="*50)
        
        return hybrid_results
    
    def _context_fusion_generation(self, query: str, vector_results: Dict[str, Any], graph_results: Dict[str, Any]) -> tuple[str, str]:
        """
        ä½¿ç”¨èåˆçš„ä¸Šä¸‹æ–‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        """
        graph_context_str = self._format_graph_results_for_prompt(graph_results)
        vector_context_str = self._format_vector_results_for_prompt(vector_results)

        fusion_prompt_template = self.langchain_rag.prompt_manager.get_prompt("fusion")
        
        final_prompt = fusion_prompt_template.format(
            question=query,
            graph_context=graph_context_str,
            vector_context=vector_context_str
        )
        
        print("--- FUSION PROMPT ---")
        print(final_prompt)
        print("---------------------")

        try:
            final_answer = self.langchain_rag.llm._call(final_prompt)
            return final_answer, final_prompt
        except Exception as e:
            print(f"âœ— ä¸Šä¸‹æ–‡èåˆç”Ÿæˆå¤±è´¥: {e}")
            return "Failed to generate a fused answer.", final_prompt

    def _calculate_vector_score(self, vector_results: Dict[str, Any], vector_weight: float) -> float:
        """
        è®¡ç®—å‘é‡æ£€ç´¢è¯„åˆ†ï¼ˆæ”¹è¿›ç‰ˆï¼‰
        """
        answer = vector_results.get('answer', '')
        
        if not answer or answer == "æ£€ç´¢å¤±è´¥":
            return 0.0
        
        # åŸºç¡€åˆ†æ•°
        base_score = vector_weight
        
        # è´¨é‡åŠ åˆ†ï¼ˆåŸºäºç­”æ¡ˆé•¿åº¦ã€å…³é”®è¯åŒ¹é…ç­‰ï¼‰
        quality_bonus = 0.0
        
        # ç­”æ¡ˆé•¿åº¦åŠ åˆ†ï¼ˆé€‚ä¸­çš„é•¿åº¦ï¼‰
        answer_length = len(answer)
        if 100 <= answer_length <= 1000:
            quality_bonus += 0.1
        elif answer_length > 1000:
            quality_bonus += 0.05
        
        # å…³é”®è¯åŒ¹é…åŠ åˆ†
        query_terms = set(vector_results.get('domain_terms', []))
        if query_terms:
            quality_bonus += 0.1
        
        return min(vector_weight + quality_bonus, vector_weight * 1.5)
    
    def _calculate_graph_score(self, graph_results: Dict[str, Any], graph_weight: float) -> float:
        """
        è®¡ç®—å›¾è°±æ£€ç´¢è¯„åˆ†ï¼ˆæ”¹è¿›ç‰ˆï¼‰
        """
        entities_found = graph_results.get('entities_found', 0)
        relationships = graph_results.get('relationships', [])
        
        if entities_found == 0:
            return 0.0
        
        # åŸºç¡€åˆ†æ•°
        base_score = graph_weight * min(entities_found, 10)  # é™åˆ¶æœ€å¤§å®ä½“æ•°
        
        # è´¨é‡åŠ åˆ†
        quality_bonus = 0.0
        
        # å…³ç³»å¤šæ ·æ€§åŠ åˆ†
        unique_relationships = set()
        for rel in relationships:
            relationship_type = rel.get('relationship', '')
            if relationship_type:
                unique_relationships.add(relationship_type)
        
        if len(unique_relationships) >= 3:
            quality_bonus += 0.2
        elif len(unique_relationships) >= 2:
            quality_bonus += 0.1
        
        # å®ä½“ç±»å‹å¤šæ ·æ€§åŠ åˆ†
        entity_types = set()
        for rel in relationships:
            source_type = rel.get('source_type', '')
            target_type = rel.get('target_type', '')
            
            # å¤„ç†source_typeå¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…å†µ
            if isinstance(source_type, list):
                if source_type:
                    entity_types.add(source_type[0])  # å–ç¬¬ä¸€ä¸ªç±»å‹
            elif source_type:
                entity_types.add(source_type)
            
            # å¤„ç†target_typeå¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…å†µ
            if isinstance(target_type, list):
                if target_type:
                    entity_types.add(target_type[0])  # å–ç¬¬ä¸€ä¸ªç±»å‹
            elif target_type:
                entity_types.add(target_type)
        
        if len(entity_types) >= 3:
            quality_bonus += 0.1
        
        return min(base_score + quality_bonus, graph_weight * 15)  # é™åˆ¶æœ€å¤§åˆ†æ•°
    
    def _assess_quality(self, vector_results: Dict[str, Any], graph_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¯„ä¼°æ£€ç´¢è´¨é‡
        """
        vector_answer = vector_results.get('answer', '')
        graph_entities = graph_results.get('entities_found', 0)
        
        # è®¡ç®—å®ä½“ç±»å‹å¤šæ ·æ€§ï¼Œå¤„ç†åˆ—è¡¨ç±»å‹
        entity_types = set()
        for rel in graph_results.get('relationships', []):
            source_type = rel.get('source_type', '')
            target_type = rel.get('target_type', '')
            
            # å¤„ç†source_typeå¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…å†µ
            if isinstance(source_type, list):
                if source_type:
                    entity_types.add(source_type[0])  # å–ç¬¬ä¸€ä¸ªç±»å‹
            elif source_type:
                entity_types.add(source_type)
            
            # å¤„ç†target_typeå¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…å†µ
            if isinstance(target_type, list):
                if target_type:
                    entity_types.add(target_type[0])  # å–ç¬¬ä¸€ä¸ªç±»å‹
            elif target_type:
                entity_types.add(target_type)
        
        quality_metrics = {
            "vector_quality": {
                "has_answer": bool(vector_answer and vector_answer != "æ£€ç´¢å¤±è´¥"),
                "answer_length": len(vector_answer) if vector_answer else 0,
                "domain_terms_matched": len(vector_results.get('domain_terms', [])),
                "quality_level": "high" if len(vector_answer) > 500 else "medium" if len(vector_answer) > 100 else "low"
            },
            "graph_quality": {
                "entities_found": graph_entities,
                "relationships_found": len(graph_results.get('relationships', [])),
                "entity_diversity": len(entity_types),
                "relationship_diversity": len(set([rel.get('relationship', '') for rel in graph_results.get('relationships', [])])),
                "quality_level": "high" if graph_entities >= 10 else "medium" if graph_entities >= 5 else "low"
            },
            "overall_quality": {
                "has_vector_answer": bool(vector_answer and vector_answer != "æ£€ç´¢å¤±è´¥"),
                "has_graph_entities": graph_entities > 0,
                "balanced_retrieval": bool(vector_answer and graph_entities > 0),
                "recommendation": self._get_quality_recommendation(vector_results, graph_results)
            }
        }
        
        return quality_metrics
    
    def _get_quality_recommendation(self, vector_results: Dict[str, Any], graph_results: Dict[str, Any]) -> str:
        """
        æ ¹æ®æ£€ç´¢ç»“æœæä¾›è´¨é‡å»ºè®®
        """
        vector_answer = vector_results.get('answer', '')
        graph_entities = graph_results.get('entities_found', 0)
        
        if vector_answer and vector_answer != "æ£€ç´¢å¤±è´¥" and graph_entities > 0:
            return "âœ… ä¼˜ç§€ï¼šå‘é‡æ£€ç´¢å’Œå›¾è°±æ£€ç´¢éƒ½æˆåŠŸ"
        elif vector_answer and vector_answer != "æ£€ç´¢å¤±è´¥":
            return "ğŸ“š è‰¯å¥½ï¼šå‘é‡æ£€ç´¢æˆåŠŸï¼Œä½†å›¾è°±æ£€ç´¢æ— ç»“æœ"
        elif graph_entities > 0:
            return "ğŸ•¸ï¸ è‰¯å¥½ï¼šå›¾è°±æ£€ç´¢æˆåŠŸï¼Œä½†å‘é‡æ£€ç´¢æ— ç»“æœ"
        else:
            return "âŒ éœ€è¦æ”¹è¿›ï¼šä¸¤ç§æ£€ç´¢æ–¹æ³•éƒ½æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"
    
    def _combine_answers(self, vector_results: Dict[str, Any], 
                        graph_results: Dict[str, Any]) -> str:
        """
        èåˆå‘é‡æ£€ç´¢å’Œå›¾è°±æ£€ç´¢çš„ç­”æ¡ˆï¼ŒåŒ…å«çŸ¥è¯†æ¥æºä¿¡æ¯
        """
        vector_answer = vector_results.get('answer', '')
        graph_entities = graph_results.get('entities_found', 0)
        graph_relationships = graph_results.get('relationships', [])
        source_documents = vector_results.get('source_documents', [])
        retrieved_docs_count = vector_results.get('retrieved_docs_count', 0)
        
        # å¦‚æœå‘é‡æ£€ç´¢æœ‰ç­”æ¡ˆï¼Œä»¥å®ƒä¸ºä¸»
        if vector_answer and vector_answer != "æ£€ç´¢å¤±è´¥":
            combined_answer = vector_answer
            
            # æ·»åŠ çŸ¥è¯†æ¥æºä¿¡æ¯
            if source_documents:
                # å¢å¼ºmetadata
                from fix_metadata_issue import enhance_source_documents_with_metadata
                enhanced_source_docs = enhance_source_documents_with_metadata(source_documents)
                
                combined_answer += f"\n\nğŸ“š çŸ¥è¯†æ¥æºä¿¡æ¯ï¼š"
                combined_answer += f"\n- æ£€ç´¢åˆ° {retrieved_docs_count} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ"
                
                # æ·»åŠ å‰3ä¸ªæ–‡æ¡£æ¥æº
                for i, doc in enumerate(enhanced_source_docs[:3]):
                    combined_answer += f"\n- æ¥æº {doc['index']}: {doc['content']}"
                    if doc.get('metadata'):
                        metadata = doc['metadata']
                        if metadata.get('source'):
                            combined_answer += f" (æ¥æº: {metadata['source']})"
                        elif metadata.get('file_name'):
                            combined_answer += f" (æ–‡ä»¶: {metadata['file_name']})"
            
            # å¦‚æœæœ‰å›¾è°±ä¿¡æ¯ï¼Œæ·»åŠ åˆ°ç­”æ¡ˆä¸­
            if graph_entities > 0:
                combined_answer += f"\n\nğŸ“Š çŸ¥è¯†å›¾è°±è¡¥å……ä¿¡æ¯ï¼š"
                combined_answer += f"\n- å‘ç° {graph_entities} ä¸ªç›¸å…³å®ä½“"
                
                # æ·»åŠ å‰3ä¸ªå…³ç³»
                for i, rel in enumerate(graph_relationships[:3]):
                    source = rel.get('source', '')
                    target = rel.get('target', '')
                    relationship = rel.get('relationship', '')
                    if source and target and relationship:
                        combined_answer += f"\n- {source} {relationship} {target}"
        else:
            # å¦‚æœå‘é‡æ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨å›¾è°±ä¿¡æ¯
            if graph_entities > 0:
                combined_answer = f"åŸºäºçŸ¥è¯†å›¾è°±æ£€ç´¢åˆ° {graph_entities} ä¸ªç›¸å…³å®ä½“ï¼š\n"
                for i, rel in enumerate(graph_relationships[:5]):
                    source = rel.get('source', '')
                    target = rel.get('target', '')
                    relationship = rel.get('relationship', '')
                    if source and target and relationship:
                        combined_answer += f"- {source} {relationship} {target}\n"
            else:
                combined_answer = "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        return combined_answer
    
    def _format_vector_results_for_prompt(self, vector_results: Dict[str, Any]) -> str:
        """Formats vector search results for the fusion prompt."""
        source_docs = vector_results.get('source_documents', [])
        if not source_docs:
            return "No relevant documents found."
        
        formatted_docs = []
        for i, doc in enumerate(source_docs[:3]):  # Limit to top 3 for conciseness
            source = doc['metadata'].get('source', 'Unknown Source')
            content = doc['content'].strip()
            formatted_docs.append(f"Source {i+1} ({source}):\n---\n{content}\n---")
            
        return "\n\n".join(formatted_docs)

    def _format_graph_results_for_prompt(self, graph_results: Dict[str, Any]) -> str:
        """Formats graph search results for the fusion prompt."""
        relationships = graph_results.get('relationships', [])
        if not relationships:
            return "No direct relationships found in the knowledge graph."
            
        formatted_rels = []
        for rel in relationships[:5]:  # Limit to top 5 for conciseness
            source = rel.get('source', 'N/A')
            target = rel.get('target', 'N/A')
            relationship = rel.get('relationship', 'RELATED_TO')
            formatted_rels.append(f"- {source} --[{relationship}]--> {target}")
            
        return "\n".join(formatted_rels)

    def get_entity_context(self, entity_name: str) -> Dict[str, Any]:
        """
        è·å–å®ä½“çš„å®Œæ•´ä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        # å®ä½“é“¾æ¥ä¿¡æ¯
        entity_context = self.entity_linker.get_entity_context(entity_name)
        
        # å›¾è°±ä¿¡æ¯
        graph_info = self.graph_retriever.get_entity_info(entity_name)
        
        # LangChainå‘é‡æ£€ç´¢ä¿¡æ¯
        vector_results = self.langchain_vector_search(entity_name)
        
        return {
            "entity": entity_name,
            "entity_linking": entity_context,
            "graph_info": graph_info,
            "vector_context": vector_results
        }
    
    def explain_retrieval(self, query: str) -> Dict[str, Any]:
        """
        è¯¦ç»†è§£é‡Šæ£€ç´¢è¿‡ç¨‹
        """
        hybrid_results = self.hybrid_search(query)
        
        explanation = {
            "query": query,
            "vector_explanation": {
                "method": "langchain_retrieval_qa",
                "framework": hybrid_results["vector_results"].get("framework", ""),
                "domain_terms": hybrid_results["vector_results"].get("domain_terms", []),
                "query_expansion": hybrid_results["vector_results"].get("expanded_query", ""),
                "final_query": hybrid_results["vector_results"].get("final_query", ""),
                "execution_time": hybrid_results["vector_results"].get("execution_time", 0)
            },
            "graph_explanation": {
                "method": "cypher_query",
                "cypher_used": hybrid_results["graph_results"].get("cypher_query", ""),
                "entities_found": hybrid_results["graph_results"].get("entities_found", 0),
                "extracted_entities": hybrid_results["graph_results"].get("extracted_entities", []),
                "relationships": [
                    {
                        "source": rel.get("source", ""),
                        "relationship": rel.get("relationship", ""),
                        "target": rel.get("target", "")
                    }
                    for rel in hybrid_results["graph_results"].get("relationships", [])[:5]
                ]
            },
            "combined_explanation": {
                "vector_weight": hybrid_results["combined_info"]["vector_weight"],
                "graph_weight": hybrid_results["combined_info"]["graph_weight"],
                "combined_score": hybrid_results["combined_info"]["combined_score"],
                "total_execution_time": hybrid_results["combined_info"]["execution_time"]
            }
        }
        
        return explanation
    
    def compare_retrieval_methods(self, query: str) -> Dict[str, Any]:
        """
        æ¯”è¾ƒä¸åŒæ£€ç´¢æ–¹æ³•çš„æ•ˆæœ
        """
        print(f"\nğŸ” æ¯”è¾ƒæ£€ç´¢æ–¹æ³•: {query}")
        print("="*50)
        
        # 1. ä»…LangChainæ£€ç´¢
        print("ğŸ“š ä»…LangChainæ£€ç´¢...")
        start_time = time.time()
        langchain_only = self.langchain_vector_search(query)
        langchain_time = time.time() - start_time
        
        # 2. ä»…å›¾è°±æ£€ç´¢
        print("ğŸ•¸ï¸ ä»…å›¾è°±æ£€ç´¢...")
        start_time = time.time()
        graph_only = self.graph_knowledge_search(query)
        graph_time = time.time() - start_time
        
        # 3. æ··åˆæ£€ç´¢
        print("ğŸ”„ æ··åˆæ£€ç´¢...")
        start_time = time.time()
        hybrid = self.hybrid_search(query)
        hybrid_time = time.time() - start_time
        
        comparison = {
            "query": query,
            "langchain_only": {
                "answer": langchain_only.get("answer", ""),
                "execution_time": langchain_time,
                "domain_terms": langchain_only.get("domain_terms", [])
            },
            "graph_only": {
                "entities_found": graph_only.get("entities_found", 0),
                "execution_time": graph_time,
                "relationships": graph_only.get("relationships", [])
            },
            "hybrid": {
                "answer": hybrid.get("final_answer", ""),
                "execution_time": hybrid_time,
                "combined_score": hybrid["combined_info"]["combined_score"]
            }
        }
        
        print(f"ğŸ“Š æ¯”è¾ƒç»“æœ:")
        print(f"  LangChain: {langchain_time:.2f}s, é¢†åŸŸæœ¯è¯­: {len(langchain_only.get('domain_terms', []))}")
        print(f"  å›¾è°±æ£€ç´¢: {graph_time:.2f}s, å®ä½“æ•°: {graph_only.get('entities_found', 0)}")
        print(f"  æ··åˆæ£€ç´¢: {hybrid_time:.2f}s, ç»¼åˆè¯„åˆ†: {hybrid['combined_info']['combined_score']:.2f}")
        
        return comparison 